{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51350745",
   "metadata": {},
   "source": [
    "Note: some of this code comes from the deepspin/quati github repository of Andre Martins's lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf070f6",
   "metadata": {},
   "source": [
    "## To do:\n",
    "1. Use dataloader and dataset classes for test\n",
    "2. Let the number of MLP layers and their widths at the end be easily controllable.\n",
    "3. Write a better test set evaluation wrapper.\n",
    "4. Use a closed form cts sparsemax and Gaussian mixture.\n",
    "5. Refactor numerical integration code to reuse when possible.\n",
    "6. Refactor code in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2934fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from basis_functions import GaussianBasisFunctions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle as pkl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871b4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is based on https://www.youtube.com/watch?v=PXOzkkB5eH0\n",
    "class FordADataset(Dataset):\n",
    "    def __init__(self,dset_string):\n",
    "        '''\n",
    "        @dset_string: set to \"FordA_TRAIN.tsv\" or \"FordA_TEST.tsv\"\n",
    "        '''\n",
    "        root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "        x, y = self.readucr(root_url + dset_string)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_samples = len(y)\n",
    "    \n",
    "    def readucr(self,filename):\n",
    "        data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "        y = torch.from_numpy(data[:, 0]).long()\n",
    "        y[y==-1]=0\n",
    "        x = torch.from_numpy(data[:, 1:]).float()\n",
    "        return x,y\n",
    "    \n",
    "    def __getitem__(self,index,train=True):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def n_classes(self):\n",
    "        return len(np.unique(self.y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97494667",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ae4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X,Y,i,bs):\n",
    "    batch_len = min(bs, X.shape[0] - 1 - i)\n",
    "    if bs==1:\n",
    "        batch_len = min(bs,X.shape[0]-i)\n",
    "    data = X[i:i+batch_len,:]\n",
    "    target = Y[i:i+batch_len]\n",
    "    return data,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6c5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_basis_functions(nb_basis, sigmas, device=None):\n",
    "    mu, sigma = torch.meshgrid(torch.linspace(0, 1, nb_basis // len(sigmas)),\n",
    "                               torch.Tensor(sigmas))\n",
    "    mus = mu.flatten().to(device)\n",
    "    sigmas = sigma.flatten().to(device)\n",
    "    return GaussianBasisFunctions(mus, sigmas)\n",
    "\n",
    "def create_psi(length,nb_basis,device=None):\n",
    "    psi = []\n",
    "    nb_waves = nb_basis\n",
    "    nb_waves = max(2,nb_waves)\n",
    "    psi.append(\n",
    "                add_gaussian_basis_functions(nb_waves,\n",
    "                                             sigmas=[.025,.1, .3],\n",
    "                                             # sigmas=[.03, .1, .3],\n",
    "                                             device=device)\n",
    "            )\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d123032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_G(max_length,nb_basis,device=None):\n",
    "    psis=[]\n",
    "    Gs = []\n",
    "\n",
    "    for length in range(1,max_length+1):\n",
    "        psi = create_psi(length,nb_basis,device=device)\n",
    "        shift = 1 / float(2 * length)\n",
    "        positions = torch.linspace(shift, 1 - shift, length)\n",
    "        positions = positions.unsqueeze(1).to(device)\n",
    "        all_basis = [basis_function.evaluate(positions)\n",
    "                             for basis_function in psi]\n",
    "        F = torch.cat(all_basis, dim=-1).t().to(device)\n",
    "        nb_basis = sum([len(b) for b in psi])\n",
    "        assert F.size(0) == nb_basis\n",
    "\n",
    "        # compute G with a ridge penalty\n",
    "        penalty = 5e-1\n",
    "        I = torch.eye(nb_basis).to(device)\n",
    "        G = F.t().matmul((F.matmul(F.t()) + penalty * I).inverse())\n",
    "        psis.append(psi)\n",
    "        Gs.append(G)\n",
    "    G = Gs[max_length-1].to(device)\n",
    "    return G,F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6379245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(torch.nn.Module):\n",
    "    def __init__(self, input_size,n_classes):\n",
    "        super(Conv, self).__init__()\n",
    "        pool_size = 2\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(1,24,5,padding='same')\n",
    "        self.conv2 = torch.nn.Conv1d(24,24,5,padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        global_conv = 1\n",
    "        #two convs, 16 filters\n",
    "        convolved = self.relu(self.conv1(x))\n",
    "        convolved = self.relu(self.conv2(convolved))\n",
    "        return convolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4edebd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"RNN module(cell type lstm or gru)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hid_size,\n",
    "        num_rnn_layers=1,\n",
    "        dropout_p = 0.2,\n",
    "        bidirectional = False,\n",
    "        rnn_type = 'lstm',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn_layer = nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hid_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout_p if num_rnn_layers>1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            self.rnn_layer = nn.GRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hid_size,\n",
    "                num_layers=num_rnn_layers,\n",
    "                dropout=dropout_p if num_rnn_layers>1 else 0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True,\n",
    "            )\n",
    "    def forward(self, input):\n",
    "        outputs, hidden_states = self.rnn_layer(input)\n",
    "        return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6b91b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAttn(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hid_size,\n",
    "        rnn_type,\n",
    "        bidirectional,\n",
    "        dx=None,\n",
    "        bandwidth=None,\n",
    "        nb_basis=32,\n",
    "        n_classes=5,\n",
    "        attn_type='cts_softmax',\n",
    "        inducing_points = 24,\n",
    "        device=None,\n",
    "        components = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_type = attn_type\n",
    "        G,F = get_G(input_size,nb_basis,device=device)\n",
    "        #G: 1 x input_size x nb_basis\n",
    "        self.G = G.unsqueeze(0)\n",
    "        self.F = F\n",
    "        self.a = 0\n",
    "        self.b = 1\n",
    "        self.components = components\n",
    "        \n",
    "        if bandwidth==None:\n",
    "            self.bandwidth = 5./input_size\n",
    "        else:\n",
    "            self.bandwidth = bandwidth\n",
    "        if dx==None:\n",
    "            self.dx = input_size\n",
    "        else:\n",
    "            self.dx = dx\n",
    "        GB = add_gaussian_basis_functions(nb_basis,sigmas=[0.025,.1, .5])\n",
    "        self.mu_basis = GB.mu\n",
    "        self.sigma_basis = GB.sigma\n",
    "        self.inducing_points = inducing_points\n",
    "        self.threshold = 1.5\n",
    "        \n",
    "        self.rnn_layer = RNN(\n",
    "            input_size=24,#hid_size * 2 if bidirectional else hid_size,\n",
    "            hid_size=hid_size,\n",
    "            rnn_type=rnn_type,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.conv = Conv(input_size,n_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.plus = torch.nn.ReLU()\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d((1))\n",
    "        \n",
    "        self.attn_fc1 = nn.Linear(hid_size, hid_size, bias=True)\n",
    "        self.attn_fc2 = nn.Linear(hid_size,1,bias=True)\n",
    "        self.attn_softmax = torch.nn.Softmax(dim=1)\n",
    "        self.inducing_map = nn.Identity(input_size,inducing_points)\n",
    "        \n",
    "        if components>0:\n",
    "            self.encode_pi1 = torch.nn.Linear(input_size, components)\n",
    "            self.encode_mu = torch.nn.Linear(input_size, components)\n",
    "            self.encode_sigma_sq1 = torch.nn.Linear(input_size, components)\n",
    "            self.encode_sigma_sq2 = torch.nn.Softplus()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=hid_size, out_features=2*hid_size)\n",
    "        self.fc2 = nn.Linear(in_features=2*hid_size, out_features=hid_size)\n",
    "        self.fc3 = nn.Linear(in_features=hid_size, out_features=n_classes)\n",
    "    \n",
    "    def update_device(self,device):\n",
    "        self.G = self.G.to(device)\n",
    "        self.F = self.F.to(device)\n",
    "        self.mu_basis = self.mu_basis.to(device)\n",
    "        self.sigma_basis = self.sigma_basis.to(device)\n",
    "        \n",
    "    def get_alpha(self,unscaled_attn):\n",
    "        return unscaled_attn\n",
    "    #torch.clamp(unscaled_attn,max=1)+torch.clamp(torch.sqrt(self.relu(unscaled_attn)),max=5)\n",
    "\n",
    "    def attention_weights(self,input):\n",
    "        #x: bs x features x T\n",
    "        x = self.conv(input)\n",
    "        #x: bs x T x features\n",
    "        x = torch.transpose(x,1,2)\n",
    "        #x_out: bs x T x hid_size\n",
    "        x_out, _ = self.rnn_layer(x)\n",
    "        unscaled_attn = self.attn_fc1(x_out)\n",
    "        unscaled_attn = torch.tanh(unscaled_attn)\n",
    "        unscaled_attn = self.attn_fc2(unscaled_attn)\n",
    "        attn_weights = self.attn_softmax(unscaled_attn)\n",
    "        attn_weights = torch.transpose(attn_weights,1,2)\n",
    "        if self.attn_type=='discrete':\n",
    "            return attn_weights\n",
    "        elif self.attn_type=='cts_softmax':\n",
    "            mu = torch.matmul(attn_weights,torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float())/attn_weights.shape[-1]\n",
    "            sigma_sq = torch.matmul(attn_weights,(torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float()/attn_weights.shape[-1])**2)-mu**2\n",
    "            attn_weights = self._integrate_product_of_gaussians(mu,sigma_sq,False)\n",
    "            return attn_weights\n",
    "        elif self.attn_type=='cts_sparsemax':\n",
    "            mu = torch.matmul(attn_weights,torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float())/attn_weights.shape[-1]\n",
    "            sigma_sq = torch.matmul(attn_weights,(torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float()/attn_weights.shape[-1])**2)-mu**2\n",
    "            attn_weights = self._integrate_wrt_truncated_parabaloid(mu,sigma_sq,False)\n",
    "            return attn_weights\n",
    "        elif self.attn_type=='kernel_softmax':\n",
    "            mu = torch.matmul(attn_weights,torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float())/attn_weights.shape[-1]\n",
    "            sigma_sq = torch.matmul(attn_weights,(torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float()/attn_weights.shape[-1])**2)-mu**2\n",
    "            alpha = self.get_alpha(unscaled_attn.transpose(1,2))\n",
    "            attn_weights = self._integrate_kernel_exp_wrt_gaussian(mu,sigma_sq,alpha,False)\n",
    "            return attn_weights\n",
    "        elif self.attn_type=='gaussian_mixture':\n",
    "            alpha  = self.get_alpha(unscaled_attn.transpose(1,2))\n",
    "            #Compute mu, sigma_sq\n",
    "            mu = self.encode_mu(alpha).squeeze(1)\n",
    "            sigma_sq = self.encode_sigma_sq1(alpha)\n",
    "            sigma_sq = self.encode_sigma_sq2(sigma_sq).squeeze(1)\n",
    "            #compute pi\n",
    "            pi = self.encode_pi1(alpha).squeeze(1)\n",
    "            attn_weights = self._integrate_gaussian_mixture(mu,sigma_sq,pi,False)\n",
    "            return attn_weights\n",
    "        elif self.attn_type=='kernel_sparsemax':\n",
    "            mu = torch.matmul(attn_weights,torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float())/attn_weights.shape[-1]\n",
    "            sigma_sq = torch.matmul(attn_weights,(torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float()/attn_weights.shape[-1])**2)-mu**2\n",
    "            alpha = self.get_alpha(unscaled_attn.transpose(1,2))\n",
    "            attn_weights = self._integrate_wrt_kernel_deformed(mu,sigma_sq,alpha,False)\n",
    "            return attn_weights\n",
    "        \n",
    "    def forward(self, input):\n",
    "        #x: bs x features x T\n",
    "        x = self.conv(input)\n",
    "        #x: bs x T x features\n",
    "        x = torch.transpose(x,1,2)\n",
    "        layer_norm = nn.LayerNorm(x.shape[2],elementwise_affine=False)\n",
    "        x = layer_norm(x)\n",
    "        #x_out: bs x T x hid_size\n",
    "        x_out, _ = self.rnn_layer(x)\n",
    "        layer_norm = nn.LayerNorm(x_out.shape[2],elementwise_affine=False)\n",
    "        x_out = layer_norm(x_out)\n",
    "        if torch.isnan(x_out).any():\n",
    "            print('hidden states have nans problem')\n",
    "            print(5/0)\n",
    "            x_out = torch.nan_to_num(x_out,0)\n",
    "        #first get discrete attention weights\n",
    "        unscaled_attn = self.attn_fc1(x_out)\n",
    "        unscaled_attn = torch.tanh(unscaled_attn)\n",
    "        unscaled_attn = self.attn_fc2(unscaled_attn)\n",
    "        attn_weights = self.attn_softmax(unscaled_attn)\n",
    "        \n",
    "        attn_weights = torch.transpose(attn_weights,1,2)\n",
    "        x = x_out\n",
    "        \n",
    "        if self.attn_type=='discrete':\n",
    "            #x: bs x hid_size x T\n",
    "            x = torch.transpose(x_out,1,2)\n",
    "            c = self.avgpool(x*attn_weights)\n",
    "            c = torch.transpose(c,1,2)\n",
    "        else:\n",
    "            mu = torch.matmul(attn_weights,torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float())/attn_weights.shape[-1]\n",
    "            sigma_sq = torch.matmul(attn_weights,(torch.arange(1,attn_weights.shape[-1]+1,device=attn_weights.device).float()/attn_weights.shape[-1])**2)-mu**2\n",
    "            alpha = self.get_alpha(unscaled_attn.transpose(1,2))\n",
    "            #B: bs x hid_size x nb\n",
    "            B = torch.matmul(torch.transpose(x,1,2),self.G)\n",
    "            if self.attn_type=='cts_softmax':\n",
    "                #numerical_integral: bs x nb x 1\n",
    "                numerical_integral = self._integrate_product_of_gaussians(mu,sigma_sq)\n",
    "            elif self.attn_type=='kernel_softmax':\n",
    "                numerical_integral = self._integrate_kernel_exp_wrt_gaussian(mu,sigma_sq,alpha).transpose(1,2)#.unsqueeze(-1)\n",
    "                #print(torch.max(numerical_integral))\n",
    "            elif self.attn_type=='cts_sparsemax':\n",
    "                numerical_integral = self._integrate_wrt_truncated_parabaloid(mu,sigma_sq).unsqueeze(-1)\n",
    "            elif self.attn_type=='kernel_sparsemax':\n",
    "                numerical_integral = self._integrate_wrt_kernel_deformed(mu,sigma_sq,alpha).transpose(1,2)\n",
    "            elif self.attn_type=='gaussian_mixture':\n",
    "                #Compute mu, sigma_sq\n",
    "                mu = self.encode_mu(alpha).squeeze(1)\n",
    "                sigma_sq = self.encode_sigma_sq1(alpha)\n",
    "                sigma_sq = self.encode_sigma_sq2(sigma_sq).squeeze(1)\n",
    "                #compute pi\n",
    "                pi = self.encode_pi1(alpha).squeeze(1)\n",
    "                #integrals gives shape: bs x heads x nb\n",
    "                numerical_integral = self._integrate_gaussian_mixture(mu,sigma_sq,pi).unsqueeze(-1)\n",
    "            elif self.attn_type=='None':\n",
    "                numerical_integral = self._integrate_product_of_gaussians(mu,sigma_sq)\n",
    "                x = torch.transpose(x_out,1,2)\n",
    "                c = self.avgpool(x)\n",
    "            if torch.isnan(numerical_integral).any():\n",
    "                print('numerical integral has nans problem')\n",
    "                print(5/0)\n",
    "            numerical_integral = torch.nan_to_num(numerical_integral,0)\n",
    "            c = torch.bmm(B,numerical_integral)\n",
    "            c = torch.transpose(c,1,2)\n",
    "            if torch.isnan(x).any():\n",
    "                print('output has nans problem')\n",
    "                print(5/0)\n",
    "                x = torch.nan_to_num(x,0)\n",
    "        x = self.fc1(c)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    def _phi(self,t):\n",
    "        return 1.0/math.sqrt(2*math.pi)*torch.exp(-0.5*t**2)\n",
    "    \n",
    "    def gaussian_rbf(self,t,bandwidth):\n",
    "        return torch.exp(-0.5*t**2/bandwidth)\n",
    "\n",
    "    def exp_kernel(self,t,bandwidth):\n",
    "        return torch.exp(-torch.abs(t)/bandwidth)\n",
    "    \n",
    "    def beta_exp(self,t):\n",
    "        q = 0\n",
    "        return self.plus(1+(1-q)*t)**(1./(1-q))\n",
    "        \n",
    "    def truncated_parabola(self,t,mu,sigma_sq,m,integrate=True):\n",
    "        return self.plus(1+t*(mu/(sigma_sq))-t**2/(2*sigma_sq)-m)\n",
    "        #return self.plus(-(t-mu)**2/(2*sigma_sq)+0.5*(3/(2*torch.sqrt(sigma_sq)))**(2./3.))\n",
    "    \n",
    "    def _integrate_product_of_gaussians(self,mu,sigma_sq,integrate=True):\n",
    "        sigma = torch.sqrt(self.sigma_basis.unsqueeze(-1)**2+sigma_sq.unsqueeze(-1))\n",
    "        if integrate:\n",
    "            return self._phi((mu.unsqueeze(-1)-self.mu_basis.unsqueeze(-1))/sigma_sq.unsqueeze(-1))/self.sigma_basis.unsqueeze(-1)\n",
    "        else:\n",
    "            T = torch.linspace(self.a,self.b,self.dx,device=mu.device).unsqueeze(0).unsqueeze(0)\n",
    "            return self._phi((mu.unsqueeze(-1)-T))/sigma_sq.unsqueeze(-1)\n",
    "    \n",
    "    def _integrate_wrt_truncated_parabaloid(self,mu,sigma_sq,integrate=True):\n",
    "        T = torch.linspace(self.a,self.b,self.dx,device=mu.device).unsqueeze(0).unsqueeze(0)\n",
    "        #phi1_upper: size 1 x x nb x dx\n",
    "        phi1_upper = self.mu_basis.unsqueeze(-1)-T\n",
    "        #phi1_lower: size 1 x nb x 1\n",
    "        phi1_lower = self.sigma_basis.unsqueeze(-1)\n",
    "        #phi1: size 1 x 1 x nb x dx\n",
    "        phi1 = self._phi(phi1_upper/phi1_lower)/phi1_lower\n",
    "        m = torch.max(T*(mu.unsqueeze(-1)/(sigma_sq.unsqueeze(-1)))-T**2/(2*sigma_sq.unsqueeze(-1)),-1)[0].unsqueeze(-1)\n",
    "        deformed_term = self.truncated_parabola(T,mu.unsqueeze(-1),sigma_sq.unsqueeze(-1),m)\n",
    "        unnormalized_density = deformed_term\n",
    "        Z = torch.trapz(unnormalized_density,torch.linspace(self.a,self.b,self.dx,device=mu.device),dim=-1).unsqueeze(-1)\n",
    "        numerical_integral = torch.trapz(phi1*unnormalized_density/Z,torch.linspace(self.a,self.b,self.dx,device=mu.device),dim=-1)\n",
    "        #numerical_integral = torch.nan_to_num(numerical_integral,0)\n",
    "        if integrate:\n",
    "            if torch.isnan(numerical_integral).any():\n",
    "                print('alpha')\n",
    "                print(alpha)\n",
    "                print('is alpha negative anywhere')\n",
    "                print(torch.sum(alpha<=0))\n",
    "                print('K')\n",
    "                print(K)\n",
    "                print('exp_f')\n",
    "                print(exp_f)\n",
    "                print('Z')\n",
    "                print(Z)\n",
    "                print(5/0)\n",
    "            return numerical_integral\n",
    "        else:\n",
    "            return unnormalized_density\n",
    "    \n",
    "    def _integrate_kernel_exp_wrt_gaussian(self,mu,sigma_sq,alpha,integrate=True):\n",
    "        T = torch.linspace(self.a,self.b,self.dx,device=mu.device).unsqueeze(0).unsqueeze(0)\n",
    "        inducing_locations = torch.linspace(0,1,self.inducing_points,device=mu.device)\n",
    "        #phi1_upper: size 1 x nb x dx\n",
    "        phi1_upper = self.mu_basis.unsqueeze(-1)-T\n",
    "        #phi1_lower: size 1 x 1 x nb x 1\n",
    "        phi1_lower = self.sigma_basis.unsqueeze(-1)\n",
    "        #phi1: size 1 x 1 x nb x dx\n",
    "        phi1 = self._phi(phi1_upper/phi1_lower)/phi1_lower\n",
    "        K_inputs = torch.cdist(inducing_locations.unsqueeze(-1),torch.linspace(self.a,self.b,self.dx,device=mu.device).unsqueeze(-1))\n",
    "        #K = self.gaussian_rbf(K_inputs,self.bandwidth)\n",
    "        K = self.exp_kernel(K_inputs,self.bandwidth)\n",
    "        m = torch.max(torch.matmul(alpha,K).unsqueeze(-2),-1)[0].unsqueeze(-1)\n",
    "        exp_terms = torch.exp(torch.matmul(alpha,K).unsqueeze(-2)-m)\n",
    "        #m = Sparsemax(dim=3)\n",
    "        #exp_terms = m(torch.matmul(alpha,K).unsqueeze(-2))\n",
    "        Z = torch.trapz(exp_terms,torch.linspace(self.a,self.b,self.dx,device=mu.device),dim=-1).unsqueeze(-1)\n",
    "        numerical_integral = torch.trapz(phi1*exp_terms/Z,torch.linspace(self.a,self.b,self.dx,device=mu.device),dim=-1)\n",
    "        if torch.isnan(numerical_integral).any():\n",
    "            print('had to rescale')\n",
    "            exp_terms = self.beta_exp(torch.matmul(alpha,K).unsqueeze(-2)/torch.abs(m))\n",
    "            Z = torch.trapz(exp_terms,torch.linspace(self.a,self.b,self.dx,device=mu.device),dim=-1).unsqueeze(-1)\n",
    "            numerical_integral = torch.trapz(phi1*exp_terms/Z,torch.linspace(self.a,self.b,self.dx,device=mu.device),dim=-1)\n",
    "        if integrate:\n",
    "            return numerical_integral\n",
    "        else:\n",
    "            return exp_terms\n",
    "\n",
    "    def _integrate_wrt_kernel_deformed(self,mu,sigma_sq,alpha,integrate=True):\n",
    "        T = torch.linspace(self.a,self.b,self.dx,device=mu.device).unsqueeze(0).unsqueeze(0)\n",
    "        inducing_locations = torch.linspace(0,1,self.inducing_points,device=mu.device)\n",
    "        #phi1_upper: size 1 x nb x dx\n",
    "        phi1_upper = self.mu_basis.unsqueeze(-1)-T\n",
    "        #phi1_lower: size 1 x 1 x nb x 1\n",
    "        phi1_lower = self.sigma_basis.unsqueeze(-1)\n",
    "        #phi1: size 1 x 1 x nb x dx\n",
    "        phi1 = self._phi(phi1_upper/phi1_lower)/phi1_lower\n",
    "        K_inputs = torch.cdist(inducing_locations.unsqueeze(-1),torch.linspace(self.a,self.b,self.dx,device=mu.device).unsqueeze(-1))\n",
    "        #K = self.gaussian_rbf(K_inputs,self.bandwidth)\n",
    "        K = self.exp_kernel(K_inputs,self.bandwidth)\n",
    "        m = torch.max(torch.matmul(alpha,K).unsqueeze(-2),-1)[0].unsqueeze(-1)\n",
    "        exp_terms = self.beta_exp(torch.matmul(alpha,K).unsqueeze(-2)-m)\n",
    "        #exp_terms = self.beta_exp(torch.matmul(alpha,K).unsqueeze(-2))\n",
    "        #m = Sparsemax(dim=3)\n",
    "        #exp_terms = m(torch.matmul(alpha,K).unsqueeze(-2))\n",
    "        Z = torch.trapz(exp_terms,torch.linspace(self.a,self.b,self.dx,device=mu.device),dim=-1).unsqueeze(-1)\n",
    "        numerical_integral = torch.trapz(phi1*exp_terms/Z,torch.linspace(self.a,self.b,self.dx,device=mu.device),dim=-1)\n",
    "        if integrate:\n",
    "            return numerical_integral\n",
    "        else:\n",
    "            return exp_terms\n",
    "    \n",
    "    def _integrate_gaussian_mixture(self,mu,sigma_sq,pi,integrate=True):\n",
    "        #T: size 1 x 1 x 1 x dx\n",
    "        T = torch.linspace(self.a,self.b,self.dx,device=device).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
    "        #phi1_upper: size 1 x 1 x nb x dx\n",
    "        phi1_upper = self.mu_basis.unsqueeze(-1)-T\n",
    "        #phi1_lower: size 1 x 1 x nb x 1\n",
    "        phi1_lower = self.sigma_basis.unsqueeze(-1)\n",
    "        #phi1: size 1 x 1 x nb x dx\n",
    "        phi1 = self._phi(phi1_upper/phi1_lower)/phi1_lower\n",
    "        #phi2_upper: size bs x components x 1 x dx\n",
    "        phi2_upper = mu.unsqueeze(-1).unsqueeze(-1)-T\n",
    "        #phi2_lower: size bs x components x 1 x 1\n",
    "        phi2_lower = sigma_sq.unsqueeze(-1).unsqueeze(-1).pow(0.5)\n",
    "        #phi2: size bs x components x 1 x dx\n",
    "        phi2 = self._phi(phi2_upper/phi2_lower)/phi2_lower\n",
    "        bs = phi2.shape[0]\n",
    "        phi2 = torch.reshape(phi2,(bs,self.components,1,self.dx))\n",
    "        pi = torch.reshape(pi,(bs,self.components))\n",
    "        pi = torch.softmax(pi,dim=1)\n",
    "        pi = pi.unsqueeze(-1).unsqueeze(-1)\n",
    "        phi2 = pi*phi2\n",
    "        phi2 = torch.sum(phi2, dim=1)\n",
    "        #phi1*phi2: size bs x nb x dx\n",
    "        numerical_integral = torch.trapz(phi1.squeeze(1)*phi2,torch.linspace(self.a,self.b,self.dx,device=device),dim=-1)   \n",
    "        if integrate:\n",
    "            return numerical_integral\n",
    "        else:\n",
    "            return phi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25c03308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,dataloader,epochs,device,scaler=None,lr=1e-3):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay=1e-5)\n",
    "    model.to(device,dtype=torch.float32)\n",
    "    model.update_device(device)\n",
    "    model.train() \n",
    "    for epoch in range(epochs):\n",
    "        dim = 0\n",
    "        batch = 0\n",
    "        for i, (data, target) in enumerate(dataloader):\n",
    "            data = data.to(device=device).unsqueeze(1)\n",
    "            target = target.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            \n",
    "            # Compute Loss\n",
    "            if scaler!=None:\n",
    "                with torch.cuda.amp.autocast(dtype=torch.float32,enabled=False):\n",
    "                    y_pred = model(data)\n",
    "                    loss = criterion(y_pred.squeeze(), target)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                # Updates the scale for next iteration\n",
    "                scaler.update()\n",
    "            else:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    y_pred = model(data)\n",
    "                    loss = criterion(y_pred.squeeze(), target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            if i%10==0:\n",
    "                print(loss)\n",
    "                print(i)\n",
    "                print('epoch')\n",
    "                print(epoch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "761771e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,x_test,y_test):\n",
    "    with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_test_cpu = []\n",
    "            for j in range(len(y_test)):\n",
    "                data,target = get_batch(x_test,y_test,j,1)\n",
    "                data = data.to(device).unsqueeze(1)\n",
    "                \n",
    "                prediction = model(data).argmax(dim=2).cpu()\n",
    "                y_pred.append(prediction.flatten().item())\n",
    "                y_test_cpu.append(target.item())\n",
    "                if j%100==0:\n",
    "                    print(j)\n",
    "    return y_pred, y_test_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ac7d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_test_cpu,y_pred):\n",
    "    confusion = metrics.confusion_matrix(y_test_cpu,y_pred)\n",
    "    for i in range(len(confusion)):\n",
    "        print(confusion[i,:]/np.sum(confusion[i,:]),np.sum(confusion[i,:]))\n",
    "    f1 = f1_score(y_test_cpu, y_pred, average=\"macro\")\n",
    "    print(\"Test f1 score : %s \"% f1)\n",
    "    acc = accuracy_score(y_test_cpu, y_pred)\n",
    "    print(\"Test accuracy score : %s \"% acc)\n",
    "    return f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da1e83e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/alexander.moreno/anaconda3/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809662/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6293, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "0\n",
      "epoch\n",
      "0\n",
      "tensor(0.7728, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "10\n",
      "epoch\n",
      "0\n",
      "tensor(0.6871, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "20\n",
      "epoch\n",
      "0\n",
      "tensor(0.7145, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "30\n",
      "epoch\n",
      "0\n",
      "tensor(0.7027, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "40\n",
      "epoch\n",
      "0\n",
      "tensor(0.6890, device='cuda:1', grad_fn=<NllLossBackward0>)\n",
      "50\n",
      "epoch\n",
      "0\n",
      "2.3791890144348145\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "nb_basis = 256\n",
    "epochs = 1\n",
    "j=0\n",
    "\n",
    "FordATrain = FordADataset(\"FordA_TRAIN.tsv\")\n",
    "DataLoaderTrain = DataLoader(dataset = FordATrain, batch_size = bs,shuffle = True)\n",
    "n_classes = FordATrain.n_classes()\n",
    "features, labels = next(iter(DataLoaderTrain))\n",
    "input_size = features.shape[1]\n",
    "\n",
    "dx = input_size\n",
    "components = 1#input_size\n",
    "runs = 1\n",
    "hid_sizes = [128]\n",
    "lr = 1e-3\n",
    "inducing_points = input_size\n",
    "\n",
    "rnn_types = ['lstm']\n",
    "attn_types=['cts_softmax']\n",
    "bidirectional = False\n",
    "models = []\n",
    "bandwidth = 1./input_size\n",
    "for run in range(runs):\n",
    "    for hid_size in hid_sizes:\n",
    "        for rnn_type in rnn_types:\n",
    "            for attn_type in attn_types:\n",
    "                model = RNNAttn(input_size,hid_size,rnn_type,bidirectional,dx=dx,nb_basis=nb_basis,bandwidth=bandwidth,attn_type=attn_type,device=device,inducing_points=inducing_points,components=components)\n",
    "                models.append(model)\n",
    "                model.to(device)\n",
    "\n",
    "trained_models = []\n",
    "np.set_printoptions(suppress=True)\n",
    "scaler=None\n",
    "start = time.time()\n",
    "while len(models)>0:\n",
    "    model = models.pop(0)\n",
    "    model = train_model(model,DataLoaderTrain,epochs,device,scaler=scaler,lr=lr)\n",
    "    trained_models.append(model)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27896c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_nice_names = {'cts_softmax':'Continuous Softmax','cts_sparsemax':'Continuous Sparsemax','kernel_softmax':'Kernel Softmax','kernel_sparsemax':'Kernel Sparsemax','gaussian_mixture':'Gaussian Mixture'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c642becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = 0\n",
    "# max_display = 4\n",
    "# display_class = 0\n",
    "# selections = np.random.choice(np.arange(len(y_test)),max_display)\n",
    "# i = 1\n",
    "# for k in selections:\n",
    "#     print('iterating')\n",
    "#     data = x_test[k,:].unsqueeze(0).unsqueeze(0).to(device)\n",
    "#     for model in trained_models:\n",
    "#         plt.plot(model.attention_weights(data)[0,:,:].cpu().flatten().detach().numpy()/torch.max(model.attention_weights(data)).item()*torch.max(data).item(),label=d_nice_names[attn_types[0]])\n",
    "#         plt.plot(data[0,:,:].cpu().flatten().detach().numpy(),label='Series')\n",
    "#         #plt.plot(model.attention_weights(data)[0,:,:].cpu().flatten().detach().numpy()/torch.max(model.attention_weights(data)).item(),label='Kernel Sparsemax')\n",
    "#         #plt.plot(model.attention_weights(data)[0,:,:].cpu().flatten().detach().numpy()/torch.max(model.attention_weights(data)),label='Kernel Sparsemax')\n",
    "#         plt.title('FordA: Original Series vs Rescaled Attention')\n",
    "#         plt.xlabel('Time')\n",
    "#         #plt.savefig('Kernel Sparsemax %i'%(k))\n",
    "#         plt.legend()\n",
    "#         plt.savefig('figures/forda0%i%s.png'%(i,attn_types[0]))\n",
    "#         plt.show()\n",
    "#         total+=1\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "376c4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results = []\n",
    "# for model in trained_models:\n",
    "#     y_pred,y_cpu = test_model(model,x_test,y_test)\n",
    "#     test_results.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87ac1f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accuracies = []\n",
    "# f1_scores = []\n",
    "# for y_pred in test_results:\n",
    "#     print('model')\n",
    "#     f1, acc = get_metrics(y_cpu,y_pred)\n",
    "#     f1_scores.append(f1)\n",
    "#     accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "766d6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = (f1_scores,accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36319420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('mean f1 score')\n",
    "# print(np.mean(np.array(f1_scores)))\n",
    "# print('1.96*sd')\n",
    "# print(1.96*np.std(np.array(f1_scores)))\n",
    "# print('max f1 score')\n",
    "# print(np.max(np.array(f1_scores)))\n",
    "# print('min f1 score')\n",
    "# print(np.min(np.array(f1_scores)))\n",
    "\n",
    "# print('mean accuracy')\n",
    "# print(np.mean(np.array(accuracies)))\n",
    "# print('1.96*sd')\n",
    "# print(1.96*np.std(np.array(accuracies)))\n",
    "# print('max accuracy')\n",
    "# print(np.max(np.array(accuracies)))\n",
    "# print('min accuracy')\n",
    "# print(np.min(np.array(accuracies)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a915b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('results+%s.pkl'%(attn_types[0]), 'wb')   # Pickle file is newly created where foo1.py is\n",
    "# pkl.dump(results, f)          # dump data to f\n",
    "# f.close()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "546be718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('results+%s.pkl'%(attn_types[0]), 'rb') as f:\n",
    "#      results = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70cb78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_scores, accuracies = results[0],results[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
